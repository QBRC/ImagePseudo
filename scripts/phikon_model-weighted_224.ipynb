{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76847948-ca44-426b-99f6-189c1f8f0959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import skimage.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a200f2aa-94af-44cb-b3c0-8441cdce94e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91e7f0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51a02b96-9b13-4362-9a24-81e4286844c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = \"Phikon\"\n",
    "#network = 'Resnet'\n",
    "transform_ops = [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=(0.485, 0.456, 0.406),\n",
    "                    std=(0.229, 0.224, 0.225),\n",
    "                        ),\n",
    "                    ]\n",
    "\n",
    "train_transform = transforms.Compose(transform_ops)\n",
    "\n",
    "'''\n",
    "if network == \"Phikon\":\n",
    "    transform_ops_ = [transforms.Resize((224, 224))] + transform_ops\n",
    "    train_transform = transforms.Compose(transform_ops_)\n",
    "else:\n",
    "    train_transform = transforms.Compose(transform_ops)\n",
    "'''\n",
    "train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78b9e747-0408-4abf-9b2d-4022bb12f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "from PIL import Image\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.df['image_path'][idx])\n",
    "\n",
    "        label = self.df['gt'][idx]\n",
    "        mask = label\n",
    "\n",
    "\n",
    "        #if self.transforms is not None:\n",
    "        img = self.transforms(img)\n",
    "        return img, mask\n",
    "# d = ImageDataset(df, _transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acbfc56a-e1a9-44a5-bd80-9486e8315aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, features_length, classes):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.extractor = ViTModel.from_pretrained(\"owkin/phikon\", add_pooling_layer=False)\n",
    "        for param in self.extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        self.linear1 = nn.Linear(768, 128, bias=True)\n",
    "        self.linear2 = nn.Linear(128, classes, bias=True)\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    def forward(self, x):\n",
    "        x = self.extractor(x).last_hidden_state[:, 0, :]\n",
    "        x = self.sigmoid1(x)\n",
    "        # max_values, _ = torch.topk(x, k=300, dim=1)\n",
    "        # min_values, _ = torch.topk(-x, k=300, dim=1)  # Use negative values to find minimum\n",
    "        # # # Concatenate max and min values\n",
    "        # top_values = torch.cat([max_values, -min_values], dim=1)\n",
    "        # # Flatten the tensor\n",
    "        # top_values = top_values.view(-1, 20)  # batch_size x 20\n",
    "        out = self.linear1(x)\n",
    "        out = self.sigmoid2(out)\n",
    "        #out = self.dropout(out)\n",
    "        out = self.linear2(out)\n",
    "\n",
    "        return out\n",
    "# net = MyModel(768, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae311262-18f2-4e11-85f7-14429b2078a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self, features_length, classes):\n",
    "        super(ResNetModel, self).__init__()\n",
    "        \n",
    "        self.netG = torchvision.models.resnet101( weights='DEFAULT')\n",
    "        self.netG.fc= nn.Linear(self.netG.fc.in_features, classes, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.netG(x)\n",
    "        #x = self.sigmoid1(x)\n",
    "        # max_values, _ = torch.topk(x, k=300, dim=1)\n",
    "        # min_values, _ = torch.topk(-x, k=300, dim=1)  # Use negative values to find minimum\n",
    "        # # # Concatenate max and min values\n",
    "        # top_values = torch.cat([max_values, -min_values], dim=1)\n",
    "        # # Flatten the tensor\n",
    "        # top_values = top_values.view(-1, 20)  # batch_size x 20\n",
    "        #out = self.linear1(x)\n",
    "        #out = self.sigmoid2(out)\n",
    "        #out = self.dropout(out)\n",
    "        #out = self.linear2(out)\n",
    "\n",
    "        return out\n",
    "# net = MyModel(768, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbab2269-68e8-4b8f-94b2-f095e1ac06d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch, params, scheduler=None):\n",
    "    model.train()\n",
    "    preds, gt = [], []\n",
    "    loss_val = []\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(params['device'], non_blocking=True)\n",
    "        labels = labels.to(params['device'], non_blocking=True)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_val.append(loss.item())\n",
    "        # pred = outputs.sigmoid().sum(1).detach().round().cpu().numpy()\n",
    "        # labels = labels.sum(1).detach().cpu().numpy()\n",
    "        proba = F.softmax(outputs, dim=1)\n",
    "        pred = torch.argmax(proba, dim=1)\n",
    "        \n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        \n",
    "        preds.extend(list(pred))\n",
    "        gt.extend(list(labels))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    acc = accuracy_score(preds, gt)\n",
    "    \n",
    "    print( \"Epoch: {epoch}. Train. {loss}, Acc. {accuracy}\".format(epoch=epoch, loss = np.mean(loss_val), accuracy = acc))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch, params):\n",
    "    model.eval()\n",
    "    preds, gt = [], []\n",
    "    loss_val = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(params['device'], non_blocking=True)\n",
    "            labels = labels.to(params['device'], non_blocking=True)\n",
    "    \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss_val.append(loss.item())\n",
    "            # pred = outputs.sigmoid().sum(1).detach().round().cpu().numpy()\n",
    "            # labels = labels.sum(1).detach().cpu().numpy()\n",
    "            proba = F.softmax(outputs, dim=1)\n",
    "            pred = torch.argmax(proba, dim=1)\n",
    "            pred = pred.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            \n",
    "            preds.extend(list(pred))\n",
    "            gt.extend(list(labels))\n",
    "            \n",
    "    acc = accuracy_score(preds, gt)\n",
    "\n",
    "    print( \"Epoch: {epoch}.  Val. {metric_monitor}, Acc. {accuracy}\".format(epoch=epoch, \n",
    "                                                                            metric_monitor=np.mean(loss_val),\n",
    "                                                                            accuracy = acc))\n",
    "    \n",
    "    return acc\n",
    "\n",
    "\n",
    "def inference(test_loader, model, params):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds, gt = [], []\n",
    "        for i, data in enumerate(test_loader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(params['device'], non_blocking=True)\n",
    "            labels = labels.to(params['device'], non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            # pred = outputs.sigmoid().sum(1).detach().round().cpu().numpy()\n",
    "            # labels = labels.sum(1).detach().cpu().numpy()\n",
    "            proba = F.softmax(outputs, dim=1)\n",
    "            pred = torch.argmax(proba, dim=1)\n",
    "            pred = pred.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "            preds.extend(list(pred))\n",
    "            gt.extend(list(labels))\n",
    "            \n",
    "        acc = accuracy_score(preds, gt)\n",
    "        print(confusion_matrix(gt, preds))\n",
    "    return acc, preds, gt\n",
    "    \n",
    "\n",
    "def train_and_validate(model, train_dataset, val_dataset, params, idx=None):\n",
    "    # sampler = WeightedRandomSampler(weights, num_samples=len(train_dataset), replacement=True)\n",
    "    # print(12, list(sampler))\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                               batch_size=params['batch_size'],\n",
    "                                               num_workers=params['num_workers'],\n",
    "                                               drop_last=True,\n",
    "                                               # sampler=sampler,\n",
    "                                               )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, \n",
    "                                             batch_size=params['batch_size'],\n",
    "                                             shuffle=False, \n",
    "                                             num_workers=params['num_workers'], \n",
    "                                             )\n",
    "    # {4: 290, 1: 284, 2: 170, 0: 94, 3: 6}\n",
    "    weights = torch.tensor([0.0607, 0.0184, 0.0293, 0.8695, 0.0221]).to(params['device'])\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights).to(params['device'])\n",
    "    #criterion = nn.CrossEntropyLoss().to(params['device'])\n",
    "    # criterion = nn.BCEWithLogitsLoss().to(params['device'])\n",
    "    optimizer = optim.AdamW(model.parameters(), \n",
    "                            lr=params['lr'], \n",
    "                            weight_decay=params[\"weight_decay\"])\n",
    "    \n",
    "    for epoch in range(1, params[\"epochs\"] + 1):\n",
    "        model = train(train_loader, model, criterion, optimizer, epoch, params)\n",
    "        score = validate(val_loader, model, criterion, epoch, params)\n",
    "        if score > params['min_score']:\n",
    "            params['min_score'] = score\n",
    "            if idx:\n",
    "                save = os.path.join(params['model_save_path'], 'snapshot'+str(idx))\n",
    "            else:\n",
    "                save = os.path.join(params['model_save_path'], 'snapshot')\n",
    "            if not os.path.exists(save):\n",
    "                os.makedirs(save)\n",
    "                \n",
    "            torch.save(model.state_dict(), os.path.join(save, \"schowder.pt\"))\n",
    "            torch.save(model.state_dict(), os.path.join(save, \"schowder_\" + str(epoch) + '.pt'))\n",
    "            print(\"============Save @ epoch {}============\".format(epoch))\n",
    "            \n",
    "    model.load_state_dict(torch.load(os.path.join(save, \"schowder.pt\")))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f0ede75-2809-4a6b-b32a-d61df50a7d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"epochs\"] = 100\n",
    "params['device'] = 'cuda:2'\n",
    "params['batch_size'] = 64 #phikon: 32 resnet: 20\n",
    "params['num_workers'] = 4\n",
    "params['lr'] = 0.001\n",
    "params[\"weight_decay\"] = 0.0005\n",
    "params['model_save_path'] = './models/1_grade_classification_in_NLST_phikon_224/' # + str(time.time()) + '/'\n",
    "params['train_dataset'] = 'train_224.csv'\n",
    "params['val_dataset'] = 'val_224.csv'\n",
    "params['test_dataset'] = 'test_224.csv'\n",
    "params['min_score'] = 0\n",
    "params['idx'] = 'weighted'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e50e830e-acb5-4542-b7fd-c651b848593c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at owkin/phikon were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoching: 0\n",
      "Epoch: 0.  Train. 2.240490059218099, Acc. 0.2663291139240506\n",
      "Epoch: 0.  Val. 1.8144976794719696, Acc. 0.3111111111111111\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/0_model.pt\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 1\n",
      "Epoch: 1.  Train. 1.5696651916350088, Acc. 0.3032911392405063\n",
      "Epoch: 1.  Val. 1.6836636066436768, Acc. 0.26666666666666666\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 2\n",
      "Epoch: 2.  Train. 1.623638637604252, Acc. 0.19139240506329114\n",
      "Epoch: 2.  Val. 1.6626582145690918, Acc. 0.31555555555555553\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/2_model.pt\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 3\n",
      "Epoch: 3.  Train. 1.5723367237275647, Acc. 0.30025316455696205\n",
      "Epoch: 3.  Val. 1.648670107126236, Acc. 0.4711111111111111\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 4\n",
      "Epoch: 4.  Train. 1.565217629555733, Acc. 0.3078481012658228\n",
      "Epoch: 4.  Val. 1.6408743858337402, Acc. 0.49333333333333335\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/4_model.pt\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 5\n",
      "Epoch: 5.  Train. 1.5543315602887062, Acc. 0.3189873417721519\n",
      "Epoch: 5.  Val. 1.6334238946437836, Acc. 0.49777777777777776\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 6\n",
      "Epoch: 6.  Train. 1.541833350735326, Acc. 0.3367088607594937\n",
      "Epoch: 6.  Val. 1.6308180391788483, Acc. 0.49777777777777776\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/6_model.pt\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 7\n",
      "Epoch: 7.  Train. 1.5312741494947864, Acc. 0.3331645569620253\n",
      "Epoch: 7.  Val. 1.6179525256156921, Acc. 0.49333333333333335\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 8\n",
      "Epoch: 8.  Train. 1.5137404126505698, Acc. 0.3569620253164557\n",
      "Epoch: 8.  Val. 1.609051764011383, Acc. 0.49333333333333335\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/8_model.pt\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 9\n",
      "Epoch: 9.  Train. 1.5024336307279524, Acc. 0.349873417721519\n",
      "Epoch: 9.  Val. 1.6081549227237701, Acc. 0.4577777777777778\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 10\n",
      "Epoch: 10.  Train. 1.4878433404430267, Acc. 0.34177215189873417\n",
      "Epoch: 10.  Val. 1.592170536518097, Acc. 0.4577777777777778\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/10_model.pt\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 11\n",
      "Epoch: 11.  Train. 1.4690476348323207, Acc. 0.33974683544303796\n",
      "Epoch: 11.  Val. 1.5961946845054626, Acc. 0.44\n",
      "Epoching: 12\n",
      "Epoch: 12.  Train. 1.4431404625215838, Acc. 0.3493670886075949\n",
      "Epoch: 12.  Val. 1.578152745962143, Acc. 0.43555555555555553\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/12_model.pt\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 13\n",
      "Epoch: 13.  Train. 1.4202067255973816, Acc. 0.3635443037974683\n",
      "Epoch: 13.  Val. 1.5797614753246307, Acc. 0.4622222222222222\n",
      "Epoching: 14\n",
      "Epoch: 14.  Train. 1.3980115152174426, Acc. 0.36\n",
      "Epoch: 14.  Val. 1.5741351246833801, Acc. 0.4266666666666667\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/14_model.pt\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 15\n",
      "Epoch: 15.  Train. 1.383424610860886, Acc. 0.3681012658227848\n",
      "Epoch: 15.  Val. 1.5556639432907104, Acc. 0.4533333333333333\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 16\n",
      "Epoch: 16.  Train. 1.340430575032388, Acc. 0.4070886075949367\n",
      "Epoch: 16.  Val. 1.5495162904262543, Acc. 0.4222222222222222\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/16_model.pt\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 17\n",
      "Epoch: 17.  Train. 1.3243554638278099, Acc. 0.4091139240506329\n",
      "Epoch: 17.  Val. 1.5507162511348724, Acc. 0.4666666666666667\n",
      "Epoching: 18\n",
      "Epoch: 18.  Train. 1.2884711007918082, Acc. 0.4217721518987342\n",
      "Epoch: 18.  Val. 1.537739336490631, Acc. 0.48\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/18_model.pt\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 19\n",
      "Epoch: 19.  Train. 1.258173038882594, Acc. 0.43696202531645567\n",
      "Epoch: 19.  Val. 1.5362125039100647, Acc. 0.4622222222222222\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 20\n",
      "Epoch: 20.  Train. 1.2243070544735077, Acc. 0.44253164556962027\n",
      "Epoch: 20.  Val. 1.5236598253250122, Acc. 0.47555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/20_model.pt\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 21\n",
      "Epoch: 21.  Train. 1.2051343706346327, Acc. 0.45974683544303796\n",
      "Epoch: 21.  Val. 1.5156580209732056, Acc. 0.47555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 22\n",
      "Epoch: 22.  Train. 1.173161268234253, Acc. 0.4789873417721519\n",
      "Epoch: 22.  Val. 1.5105861127376556, Acc. 0.49777777777777776\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/22_model.pt\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 23\n",
      "Epoch: 23.  Train. 1.1420854849200095, Acc. 0.4962025316455696\n",
      "Epoch: 23.  Val. 1.5101369619369507, Acc. 0.5111111111111111\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 24\n",
      "Epoch: 24.  Train. 1.1177207077703168, Acc. 0.5088607594936709\n",
      "Epoch: 24.  Val. 1.5060203075408936, Acc. 0.5111111111111111\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/24_model.pt\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 25\n",
      "Epoch: 25.  Train. 1.0927502512931824, Acc. 0.5149367088607595\n",
      "Epoch: 25.  Val. 1.5058900117874146, Acc. 0.5155555555555555\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/model_best.pt\n",
      "Epoching: 26\n",
      "Epoch: 26.  Train. 1.0699737629582804, Acc. 0.5255696202531646\n",
      "Epoch: 26.  Val. 1.5133531391620636, Acc. 0.52\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/26_model.pt\n",
      "Epoching: 27\n",
      "Epoch: 27.  Train. 1.0486051747875829, Acc. 0.529620253164557\n",
      "Epoch: 27.  Val. 1.5237886905670166, Acc. 0.5244444444444445\n",
      "Epoching: 28\n",
      "Epoch: 28.  Train. 1.0319382692537, Acc. 0.5331645569620254\n",
      "Epoch: 28.  Val. 1.5250023156404495, Acc. 0.5155555555555555\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/28_model.pt\n",
      "Epoching: 29\n",
      "Epoch: 29.  Train. 1.0213651080285349, Acc. 0.5367088607594936\n",
      "Epoch: 29.  Val. 1.5275999307632446, Acc. 0.5333333333333333\n",
      "Epoching: 30\n",
      "Epoch: 30.  Train. 1.0021875183428488, Acc. 0.5417721518987342\n",
      "Epoch: 30.  Val. 1.5274661034345627, Acc. 0.5333333333333333\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/30_model.pt\n",
      "Epoching: 31\n",
      "Epoch: 31.  Train. 0.9831601314006313, Acc. 0.5443037974683544\n",
      "Epoch: 31.  Val. 1.5370605140924454, Acc. 0.5244444444444445\n",
      "Epoching: 32\n",
      "Epoch: 32.  Train. 0.9630064483611814, Acc. 0.5534177215189874\n",
      "Epoch: 32.  Val. 1.5427706688642502, Acc. 0.5244444444444445\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/32_model.pt\n",
      "Epoching: 33\n",
      "Epoch: 33.  Train. 0.9549975847044299, Acc. 0.5534177215189874\n",
      "Epoch: 33.  Val. 1.5509238839149475, Acc. 0.5288888888888889\n",
      "Epoching: 34\n",
      "Epoch: 34.  Train. 0.9417739001012617, Acc. 0.5605063291139241\n",
      "Epoch: 34.  Val. 1.5519244968891144, Acc. 0.5333333333333333\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/34_model.pt\n",
      "Epoching: 35\n",
      "Epoch: 35.  Train. 0.929175449955848, Acc. 0.5610126582278481\n",
      "Epoch: 35.  Val. 1.5661365687847137, Acc. 0.5288888888888889\n",
      "Epoching: 36\n",
      "Epoch: 36.  Train. 0.9158900283998058, Acc. 0.5640506329113925\n",
      "Epoch: 36.  Val. 1.5733225643634796, Acc. 0.5333333333333333\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/36_model.pt\n",
      "Epoching: 37\n",
      "Epoch: 37.  Train. 0.90757833180889, Acc. 0.5670886075949367\n",
      "Epoch: 37.  Val. 1.5752679705619812, Acc. 0.5422222222222223\n",
      "Epoching: 38\n",
      "Epoch: 38.  Train. 0.8973680157815257, Acc. 0.5665822784810126\n",
      "Epoch: 38.  Val. 1.5833087414503098, Acc. 0.5377777777777778\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/38_model.pt\n",
      "Epoching: 39\n",
      "Epoch: 39.  Train. 0.889658821205939, Acc. 0.5736708860759494\n",
      "Epoch: 39.  Val. 1.5916448682546616, Acc. 0.5377777777777778\n",
      "Epoching: 40\n",
      "Epoch: 40.  Train. 0.8778621352487995, Acc. 0.5777215189873418\n",
      "Epoch: 40.  Val. 1.5979276895523071, Acc. 0.5288888888888889\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/40_model.pt\n",
      "Epoching: 41\n",
      "Epoch: 41.  Train. 0.8731898069381714, Acc. 0.5827848101265822\n",
      "Epoch: 41.  Val. 1.6020145118236542, Acc. 0.5288888888888889\n",
      "Epoching: 42\n",
      "Epoch: 42.  Train. 0.8622331157807381, Acc. 0.5863291139240506\n",
      "Epoch: 42.  Val. 1.6166232973337173, Acc. 0.5288888888888889\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/42_model.pt\n",
      "Epoching: 43\n",
      "Epoch: 43.  Train. 0.8566710074101725, Acc. 0.589873417721519\n",
      "Epoch: 43.  Val. 1.6150918304920197, Acc. 0.5333333333333333\n",
      "Epoching: 44\n",
      "Epoch: 44.  Train. 0.8477271458794994, Acc. 0.5873417721518988\n",
      "Epoch: 44.  Val. 1.629888877272606, Acc. 0.5244444444444445\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/44_model.pt\n",
      "Epoching: 45\n",
      "Epoch: 45.  Train. 0.8431704630774837, Acc. 0.5913924050632912\n",
      "Epoch: 45.  Val. 1.6288791596889496, Acc. 0.52\n",
      "Epoching: 46\n",
      "Epoch: 46.  Train. 0.8332885590291792, Acc. 0.5939240506329114\n",
      "Epoch: 46.  Val. 1.6425564140081406, Acc. 0.52\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/46_model.pt\n",
      "Epoching: 47\n",
      "Epoch: 47.  Train. 0.8288082605408084, Acc. 0.6005063291139241\n",
      "Epoch: 47.  Val. 1.6414857804775238, Acc. 0.5066666666666667\n",
      "Epoching: 48\n",
      "Epoch: 48.  Train. 0.8206901511838359, Acc. 0.6020253164556962\n",
      "Epoch: 48.  Val. 1.6559632271528244, Acc. 0.5022222222222222\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/48_model.pt\n",
      "Epoching: 49\n",
      "Epoch: 49.  Train. 0.8157639301592304, Acc. 0.6091139240506329\n",
      "Epoch: 49.  Val. 1.6530699729919434, Acc. 0.5022222222222222\n",
      "Epoching: 50\n",
      "Epoch: 50.  Train. 0.8083496290829874, Acc. 0.6096202531645569\n",
      "Epoch: 50.  Val. 1.667468622326851, Acc. 0.5022222222222222\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/50_model.pt\n",
      "Epoching: 51\n",
      "Epoch: 51.  Train. 0.803368569862458, Acc. 0.6141772151898734\n",
      "Epoch: 51.  Val. 1.6647339761257172, Acc. 0.5066666666666667\n",
      "Epoching: 52\n",
      "Epoch: 52.  Train. 0.796445601890164, Acc. 0.6167088607594937\n",
      "Epoch: 52.  Val. 1.678145170211792, Acc. 0.5022222222222222\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/52_model.pt\n",
      "Epoching: 53\n",
      "Epoch: 53.  Train. 0.7916543565450176, Acc. 0.6207594936708861\n",
      "Epoch: 53.  Val. 1.6755305528640747, Acc. 0.5244444444444445\n",
      "Epoching: 54\n",
      "Epoch: 54.  Train. 0.7856500153580019, Acc. 0.6237974683544304\n",
      "Epoch: 54.  Val. 1.688254326581955, Acc. 0.52\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/54_model.pt\n",
      "Epoching: 55\n",
      "Epoch: 55.  Train. 0.7805611894976708, Acc. 0.6248101265822785\n",
      "Epoch: 55.  Val. 1.6861720085144043, Acc. 0.5244444444444445\n",
      "Epoching: 56\n",
      "Epoch: 56.  Train. 0.7746887687713869, Acc. 0.6283544303797468\n",
      "Epoch: 56.  Val. 1.6983617693185806, Acc. 0.5244444444444445\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/56_model.pt\n",
      "Epoching: 57\n",
      "Epoch: 57.  Train. 0.7700978584827916, Acc. 0.6308860759493671\n",
      "Epoch: 57.  Val. 1.6961633265018463, Acc. 0.5244444444444445\n",
      "Epoching: 58\n",
      "Epoch: 58.  Train. 0.7641729010689643, Acc. 0.6349367088607595\n",
      "Epoch: 58.  Val. 1.707035169005394, Acc. 0.5244444444444445\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/58_model.pt\n",
      "Epoching: 59\n",
      "Epoch: 59.  Train. 0.7596926405545203, Acc. 0.6384810126582279\n",
      "Epoch: 59.  Val. 1.7060704976320267, Acc. 0.5422222222222223\n",
      "Epoching: 60\n",
      "Epoch: 60.  Train. 0.7543940342241718, Acc. 0.6445569620253164\n",
      "Epoch: 60.  Val. 1.715853989124298, Acc. 0.5422222222222223\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/60_model.pt\n",
      "Epoching: 61\n",
      "Epoch: 61.  Train. 0.7488720561227491, Acc. 0.6445569620253164\n",
      "Epoch: 61.  Val. 1.714963674545288, Acc. 0.5288888888888889\n",
      "Epoching: 62\n",
      "Epoch: 62.  Train. 0.7441891763479479, Acc. 0.6470886075949367\n",
      "Epoch: 62.  Val. 1.7238903045654297, Acc. 0.5377777777777778\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/62_model.pt\n",
      "Epoching: 63\n",
      "Epoch: 63.  Train. 0.7398756074328576, Acc. 0.6475949367088608\n",
      "Epoch: 63.  Val. 1.7256013453006744, Acc. 0.5244444444444445\n",
      "Epoching: 64\n",
      "Epoch: 64.  Train. 0.7349513480740208, Acc. 0.649620253164557\n",
      "Epoch: 64.  Val. 1.7329301685094833, Acc. 0.5288888888888889\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/64_model.pt\n",
      "Epoching: 65\n",
      "Epoch: 65.  Train. 0.7294915895308217, Acc. 0.6511392405063291\n",
      "Epoch: 65.  Val. 1.73842391371727, Acc. 0.5333333333333333\n",
      "Epoching: 66\n",
      "Epoch: 66.  Train. 0.7259780764579773, Acc. 0.650126582278481\n",
      "Epoch: 66.  Val. 1.7417119294404984, Acc. 0.5422222222222223\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/66_model.pt\n",
      "Epoching: 67\n",
      "Epoch: 67.  Train. 0.7206620703781804, Acc. 0.6541772151898734\n",
      "Epoch: 67.  Val. 1.745114803314209, Acc. 0.5466666666666666\n",
      "Epoching: 68\n",
      "Epoch: 68.  Train. 0.7157842516899109, Acc. 0.6587341772151899\n",
      "Epoch: 68.  Val. 1.7520054578781128, Acc. 0.5377777777777778\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/68_model.pt\n",
      "Epoching: 69\n",
      "Epoch: 69.  Train. 0.7107302839717557, Acc. 0.660759493670886\n",
      "Epoch: 69.  Val. 1.7548156529664993, Acc. 0.5466666666666666\n",
      "Epoching: 70\n",
      "Epoch: 70.  Train. 0.7069864446117032, Acc. 0.6637974683544304\n",
      "Epoch: 70.  Val. 1.7607423067092896, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/70_model.pt\n",
      "Epoching: 71\n",
      "Epoch: 71.  Train. 0.7022989060609571, Acc. 0.6637974683544304\n",
      "Epoch: 71.  Val. 1.7631897330284119, Acc. 0.5511111111111111\n",
      "Epoching: 72\n",
      "Epoch: 72.  Train. 0.6970432285339602, Acc. 0.6683544303797468\n",
      "Epoch: 72.  Val. 1.7725645303726196, Acc. 0.5466666666666666\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/72_model.pt\n",
      "Epoching: 73\n",
      "Epoch: 73.  Train. 0.6931977305681475, Acc. 0.6678481012658228\n",
      "Epoch: 73.  Val. 1.7721919268369675, Acc. 0.5555555555555556\n",
      "Epoching: 74\n",
      "Epoch: 74.  Train. 0.6888958951157909, Acc. 0.6718987341772152\n",
      "Epoch: 74.  Val. 1.7824592888355255, Acc. 0.56\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/74_model.pt\n",
      "Epoching: 75\n",
      "Epoch: 75.  Train. 0.6840390841807088, Acc. 0.669367088607595\n",
      "Epoch: 75.  Val. 1.7825479805469513, Acc. 0.56\n",
      "Epoching: 76\n",
      "Epoch: 76.  Train. 0.6800619921376628, Acc. 0.6739240506329114\n",
      "Epoch: 76.  Val. 1.7937096804380417, Acc. 0.5688888888888889\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/76_model.pt\n",
      "Epoching: 77\n",
      "Epoch: 77.  Train. 0.6760202381880053, Acc. 0.6759493670886076\n",
      "Epoch: 77.  Val. 1.7911621779203415, Acc. 0.56\n",
      "Epoching: 78\n",
      "Epoch: 78.  Train. 0.670331469947292, Acc. 0.68\n",
      "Epoch: 78.  Val. 1.8077492862939835, Acc. 0.5644444444444444\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/78_model.pt\n",
      "Epoching: 79\n",
      "Epoch: 79.  Train. 0.6678959789776033, Acc. 0.6794936708860759\n",
      "Epoch: 79.  Val. 1.7995300889015198, Acc. 0.5644444444444444\n",
      "Epoching: 80\n",
      "Epoch: 80.  Train. 0.6614929000216145, Acc. 0.6860759493670886\n",
      "Epoch: 80.  Val. 1.8162693977355957, Acc. 0.5733333333333334\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/80_model.pt\n",
      "Epoching: 81\n",
      "Epoch: 81.  Train. 0.6590472921248405, Acc. 0.6855696202531646\n",
      "Epoch: 81.  Val. 1.8112097382545471, Acc. 0.5777777777777777\n",
      "Epoching: 82\n",
      "Epoch: 82.  Train. 0.6528305680521073, Acc. 0.6891139240506329\n",
      "Epoch: 82.  Val. 1.82876655459404, Acc. 0.5777777777777777\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/82_model.pt\n",
      "Epoching: 83\n",
      "Epoch: 83.  Train. 0.6509882370310445, Acc. 0.6865822784810126\n",
      "Epoch: 83.  Val. 1.8230963498353958, Acc. 0.5777777777777777\n",
      "Epoching: 84\n",
      "Epoch: 84.  Train. 0.6443517775304856, Acc. 0.6946835443037974\n",
      "Epoch: 84.  Val. 1.8394858688116074, Acc. 0.5866666666666667\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/84_model.pt\n",
      "Epoching: 85\n",
      "Epoch: 85.  Train. 0.6420934637708049, Acc. 0.690632911392405\n",
      "Epoch: 85.  Val. 1.8340004086494446, Acc. 0.5733333333333334\n",
      "Epoching: 86\n",
      "Epoch: 86.  Train. 0.6363341957330704, Acc. 0.6977215189873418\n",
      "Epoch: 86.  Val. 1.8519990146160126, Acc. 0.5822222222222222\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/86_model.pt\n",
      "Epoching: 87\n",
      "Epoch: 87.  Train. 0.6336943723501698, Acc. 0.6931645569620253\n",
      "Epoch: 87.  Val. 1.8469183444976807, Acc. 0.5777777777777777\n",
      "Epoching: 88\n",
      "Epoch: 88.  Train. 0.6277837383170282, Acc. 0.7012658227848101\n",
      "Epoch: 88.  Val. 1.8640537858009338, Acc. 0.5822222222222222\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/88_model.pt\n",
      "Epoching: 89\n",
      "Epoch: 89.  Train. 0.6251305904119245, Acc. 0.6962025316455697\n",
      "Epoch: 89.  Val. 1.8595042526721954, Acc. 0.5733333333333334\n",
      "Epoching: 90\n",
      "Epoch: 90.  Train. 0.6194582019121416, Acc. 0.7037974683544304\n",
      "Epoch: 90.  Val. 1.8768935799598694, Acc. 0.5777777777777777\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/90_model.pt\n",
      "Epoching: 91\n",
      "Epoch: 91.  Train. 0.6165434852723153, Acc. 0.6992405063291139\n",
      "Epoch: 91.  Val. 1.8731974810361862, Acc. 0.5777777777777777\n",
      "Epoching: 92\n",
      "Epoch: 92.  Train. 0.6110589869560734, Acc. 0.7113924050632912\n",
      "Epoch: 92.  Val. 1.8904031068086624, Acc. 0.5822222222222222\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/92_model.pt\n",
      "Epoching: 93\n",
      "Epoch: 93.  Train. 0.6079410074218627, Acc. 0.7027848101265822\n",
      "Epoch: 93.  Val. 1.8873302340507507, Acc. 0.5822222222222222\n",
      "Epoching: 94\n",
      "Epoch: 94.  Train. 0.6026241519758778, Acc. 0.7134177215189873\n",
      "Epoch: 94.  Val. 1.9044899642467499, Acc. 0.5822222222222222\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/94_model.pt\n",
      "Epoching: 95\n",
      "Epoch: 95.  Train. 0.599267759630757, Acc. 0.7088607594936709\n",
      "Epoch: 95.  Val. 1.901667833328247, Acc. 0.5822222222222222\n",
      "Epoching: 96\n",
      "Epoch: 96.  Train. 0.5941175251237808, Acc. 0.7179746835443038\n",
      "Epoch: 96.  Val. 1.9190043061971664, Acc. 0.5911111111111111\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/96_model.pt\n",
      "Epoching: 97\n",
      "Epoch: 97.  Train. 0.5904458612203598, Acc. 0.7164556962025317\n",
      "Epoch: 97.  Val. 1.9161279201507568, Acc. 0.5866666666666667\n",
      "Epoching: 98\n",
      "Epoch: 98.  Train. 0.5855459998692235, Acc. 0.7220253164556962\n",
      "Epoch: 98.  Val. 1.9334703087806702, Acc. 0.5911111111111111\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/98_model.pt\n",
      "Epoching: 99\n",
      "Epoch: 99.  Train. 0.5819846055200023, Acc. 0.72\n",
      "Epoch: 99.  Val. 1.9316611289978027, Acc. 0.5955555555555555\n",
      "Epoching: 100\n",
      "Epoch: 100.  Train. 0.5769410186236904, Acc. 0.7240506329113924\n",
      "Epoch: 100.  Val. 1.9483683109283447, Acc. 0.6\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/100_model.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df = pd.read_csv(params['train_dataset'])\n",
    "val_df = pd.read_csv(params['val_dataset'])\n",
    "test_df = pd.read_csv(params['test_dataset'])\n",
    "    \n",
    "train_dataset = ImageDataset(train_df, train_transform)\n",
    "val_dataset = ImageDataset(val_df, train_transform)\n",
    "test_dataset = ImageDataset(test_df, train_transform)\n",
    "\n",
    "#train_dataset = ImageDataset(train_df, transforms=None)\n",
    "#val_dataset = ImageDataset(val_df, transforms=None)\n",
    "#test_dataset = ImageDataset(test_df, transforms=None)\n",
    "\n",
    "    \n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                             batch_size=params['batch_size'],\n",
    "                                             shuffle=False, \n",
    "                                             num_workers=params['num_workers'], \n",
    "                                             )\n",
    "    \n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, \n",
    "                                             batch_size=params['batch_size'],\n",
    "                                             shuffle=False, \n",
    "                                             num_workers=params['num_workers'], \n",
    "                                             )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                             batch_size=params['batch_size'],\n",
    "                                             shuffle=False, \n",
    "                                             num_workers=params['num_workers'], \n",
    "                                             )\n",
    "\n",
    "model = MyModel(768, 5)\n",
    "model = model.to(params[\"device\"])\n",
    "weights = torch.tensor([0.0607, 0.0184, 0.0293, 0.8695, 0.0221]).to(params['device'])\n",
    "criterion = nn.CrossEntropyLoss(weight=weights).to(params['device'])\n",
    "optimizer = optim.AdamW(model.parameters(), \n",
    "                            lr=params['lr'], \n",
    "                            weight_decay=params[\"weight_decay\"])\n",
    "\n",
    "hist_train_loss = []\n",
    "hist_val_loss = []\n",
    "best_loss_val = 10000\n",
    "\n",
    "save_freq = 2\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(params[\"epochs\"] + 1):\n",
    "    epoch_loss = 0\n",
    "    val_loss = 0\n",
    "    preds, gt = [], []\n",
    "    print('Epoching:', epoch)\n",
    "    \n",
    "    model.train() \n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = data\n",
    "        #print(inputs, labels)\n",
    "        inputs = inputs.to(params['device'], non_blocking=True)\n",
    "        labels = labels.to(params['device'], non_blocking=True)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        #print(loss.item())\n",
    "        \n",
    "        proba = F.softmax(outputs, dim=1)\n",
    "        pred = torch.argmax(proba, dim=1)\n",
    "        \n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        \n",
    "        preds.extend(list(pred))\n",
    "        gt.extend(list(labels))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    acc = accuracy_score(preds, gt)\n",
    "    #print(gt)\n",
    "    avg_loss_train = epoch_loss / len(train_loader)\n",
    "    hist_train_loss.append(avg_loss_train)\n",
    "    print( \"Epoch: {epoch}.  Train. {metric_monitor}, Acc. {accuracy}\".format(epoch=epoch, \n",
    "                                                                            metric_monitor=avg_loss_train,\n",
    "                                                                            accuracy = acc))\n",
    "    preds, gt = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            optimizer.zero_grad()\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(params['device'], non_blocking=True)\n",
    "            labels = labels.to(params['device'], non_blocking=True)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "        \n",
    "            proba = F.softmax(outputs, dim=1)\n",
    "            pred = torch.argmax(proba, dim=1)\n",
    "        \n",
    "            pred = pred.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "        \n",
    "            preds.extend(list(pred))\n",
    "            gt.extend(list(labels))\n",
    "        \n",
    "    \n",
    "        acc = accuracy_score(preds, gt)\n",
    "        avg_loss_val = val_loss / len(val_loader)\n",
    "        hist_val_loss.append(avg_loss_val)\n",
    "        print( \"Epoch: {epoch}.  Val. {metric_monitor}, Acc. {accuracy}\".format(epoch=epoch, \n",
    "                                                                            metric_monitor=avg_loss_val,\n",
    "                                                                            accuracy = acc))\n",
    "    \n",
    "\n",
    "    if epoch % save_freq == 0:\n",
    "        torch.save(model.state_dict(),os.path.join(params['model_save_path'],str(epoch)+\"_model.pt\"))\n",
    "        print(\"Saved model to {}\".format(os.path.join(params['model_save_path'], str(epoch)+\"_model.pt\")))\n",
    "    #print(best_loss_val)\n",
    "    if hist_val_loss[-1] < best_loss_val:\n",
    "        torch.save(model.state_dict(),os.path.join(params['model_save_path'],\"model_best.pt\"))\n",
    "        print(\"Saved model to {}\".format(os.path.join(params['model_save_path'], \"model_best.pt\")))\n",
    "        best_loss_val = hist_val_loss[-1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "89f1a082-e37b-41c6-9637-7d4b528d2aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at owkin/phikon were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (extractor): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (sigmoid1): Sigmoid()\n",
       "  (linear1): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=5, bias=True)\n",
       "  (sigmoid2): Sigmoid()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save = os.path.join(params['model_save_path'])\n",
    "model=MyModel(768,5)\n",
    "model = model.to(params[\"device\"])\n",
    "model.load_state_dict(torch.load(os.path.join(save, \"100_model.pt\")))\n",
    "weights = torch.tensor([0.0607, 0.0184, 0.0293, 0.8695, 0.0221]).to(params['device'])\n",
    "criterion = nn.CrossEntropyLoss(weight=weights).to(params['device'])\n",
    "optimizer = optim.AdamW(model.parameters(), \n",
    "                            lr=params['lr'], \n",
    "                            weight_decay=params[\"weight_decay\"])\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de8c861c-6a0f-4dbc-9309-6064233c51b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoching: 101\n",
      "Epoch: 101.  Train. 0.6243859754214364, Acc. 0.7012658227848101\n",
      "Epoch: 101.  Val. 2.0394673943519592, Acc. 0.56\n",
      "Epoching: 102\n",
      "Epoch: 102.  Train. 0.6863223718058679, Acc. 0.6835443037974683\n",
      "Epoch: 102.  Val. 1.945836216211319, Acc. 0.5911111111111111\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/102_model.pt\n",
      "Epoching: 103\n",
      "Epoch: 103.  Train. 0.5362324414234008, Acc. 0.7367088607594937\n",
      "Epoch: 103.  Val. 1.976904422044754, Acc. 0.5866666666666667\n",
      "Epoching: 104\n",
      "Epoch: 104.  Train. 0.5432578685783571, Acc. 0.739240506329114\n",
      "Epoch: 104.  Val. 2.0472925305366516, Acc. 0.5733333333333334\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/104_model.pt\n",
      "Epoching: 105\n",
      "Epoch: 105.  Train. 0.5517358904884707, Acc. 0.7321518987341772\n",
      "Epoch: 105.  Val. 2.0416150987148285, Acc. 0.5733333333333334\n",
      "Epoching: 106\n",
      "Epoch: 106.  Train. 0.5442118846601055, Acc. 0.7407594936708861\n",
      "Epoch: 106.  Val. 2.07174551486969, Acc. 0.5733333333333334\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/106_model.pt\n",
      "Epoching: 107\n",
      "Epoch: 107.  Train. 0.5438688479123577, Acc. 0.739746835443038\n",
      "Epoch: 107.  Val. 2.0904574245214462, Acc. 0.5822222222222222\n",
      "Epoching: 108\n",
      "Epoch: 108.  Train. 0.5413568322696993, Acc. 0.7422784810126583\n",
      "Epoch: 108.  Val. 2.109912469983101, Acc. 0.5822222222222222\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/108_model.pt\n",
      "Epoching: 109\n",
      "Epoch: 109.  Train. 0.5383924442914224, Acc. 0.7422784810126583\n",
      "Epoch: 109.  Val. 2.1298844516277313, Acc. 0.5866666666666667\n",
      "Epoching: 110\n",
      "Epoch: 110.  Train. 0.5354972246193117, Acc. 0.7437974683544304\n",
      "Epoch: 110.  Val. 2.148364380002022, Acc. 0.5866666666666667\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/110_model.pt\n",
      "Epoching: 111\n",
      "Epoch: 111.  Train. 0.5321661209867846, Acc. 0.7463291139240507\n",
      "Epoch: 111.  Val. 2.167369544506073, Acc. 0.5866666666666667\n",
      "Epoching: 112\n",
      "Epoch: 112.  Train. 0.5287758174442476, Acc. 0.7503797468354431\n",
      "Epoch: 112.  Val. 2.183873400092125, Acc. 0.5866666666666667\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/112_model.pt\n",
      "Epoching: 113\n",
      "Epoch: 113.  Train. 0.5249685947933505, Acc. 0.7524050632911392\n",
      "Epoch: 113.  Val. 2.206458181142807, Acc. 0.5866666666666667\n",
      "Epoching: 114\n",
      "Epoch: 114.  Train. 0.5219178204574892, Acc. 0.7564556962025316\n",
      "Epoch: 114.  Val. 2.2196213752031326, Acc. 0.5866666666666667\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/114_model.pt\n",
      "Epoching: 115\n",
      "Epoch: 115.  Train. 0.5181006601741237, Acc. 0.7569620253164557\n",
      "Epoch: 115.  Val. 2.2397557348012924, Acc. 0.5822222222222222\n",
      "Epoching: 116\n",
      "Epoch: 116.  Train. 0.5143897792985362, Acc. 0.7625316455696203\n",
      "Epoch: 116.  Val. 2.2553762197494507, Acc. 0.5866666666666667\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/116_model.pt\n",
      "Epoching: 117\n",
      "Epoch: 117.  Train. 0.5108445195421096, Acc. 0.7635443037974684\n",
      "Epoch: 117.  Val. 2.272328808903694, Acc. 0.5866666666666667\n",
      "Epoching: 118\n",
      "Epoch: 118.  Train. 0.5067817741824735, Acc. 0.7655696202531646\n",
      "Epoch: 118.  Val. 2.293871656060219, Acc. 0.5866666666666667\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/118_model.pt\n",
      "Epoching: 119\n",
      "Epoch: 119.  Train. 0.5031463258689449, Acc. 0.7670886075949367\n",
      "Epoch: 119.  Val. 2.303247094154358, Acc. 0.5866666666666667\n",
      "Epoching: 120\n",
      "Epoch: 120.  Train. 0.4992595680298344, Acc. 0.769113924050633\n",
      "Epoch: 120.  Val. 2.3233349919319153, Acc. 0.5822222222222222\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/120_model.pt\n",
      "Epoching: 121\n",
      "Epoch: 121.  Train. 0.49542932741103635, Acc. 0.7711392405063291\n",
      "Epoch: 121.  Val. 2.3352728486061096, Acc. 0.5822222222222222\n",
      "Epoching: 122\n",
      "Epoch: 122.  Train. 0.49164169810471997, Acc. 0.7731645569620254\n",
      "Epoch: 122.  Val. 2.3367884904146194, Acc. 0.5822222222222222\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/122_model.pt\n",
      "Epoching: 123\n",
      "Epoch: 123.  Train. 0.4868768338234194, Acc. 0.7762025316455696\n",
      "Epoch: 123.  Val. 2.369273528456688, Acc. 0.5777777777777777\n",
      "Epoching: 124\n",
      "Epoch: 124.  Train. 0.48450486097605, Acc. 0.7777215189873418\n",
      "Epoch: 124.  Val. 2.385675296187401, Acc. 0.5777777777777777\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/124_model.pt\n",
      "Epoching: 125\n",
      "Epoch: 125.  Train. 0.4799279599420486, Acc. 0.779746835443038\n",
      "Epoch: 125.  Val. 2.398557484149933, Acc. 0.5822222222222222\n",
      "Epoching: 126\n",
      "Epoch: 126.  Train. 0.47614888318123355, Acc. 0.780253164556962\n",
      "Epoch: 126.  Val. 2.420086443424225, Acc. 0.5822222222222222\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/126_model.pt\n",
      "Epoching: 127\n",
      "Epoch: 127.  Train. 0.47241760093358254, Acc. 0.780759493670886\n",
      "Epoch: 127.  Val. 2.4297273010015488, Acc. 0.5822222222222222\n",
      "Epoching: 128\n",
      "Epoch: 128.  Train. 0.46828961780955713, Acc. 0.7832911392405063\n",
      "Epoch: 128.  Val. 2.453204318881035, Acc. 0.5822222222222222\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/128_model.pt\n",
      "Epoching: 129\n",
      "Epoch: 129.  Train. 0.46448090360049277, Acc. 0.7858227848101266\n",
      "Epoch: 129.  Val. 2.459260955452919, Acc. 0.5777777777777777\n",
      "Epoching: 130\n",
      "Epoch: 130.  Train. 0.4606523823834235, Acc. 0.7888607594936708\n",
      "Epoch: 130.  Val. 2.4834515005350113, Acc. 0.5777777777777777\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/130_model.pt\n",
      "Epoching: 131\n",
      "Epoch: 131.  Train. 0.45669691817414376, Acc. 0.7929113924050633\n",
      "Epoch: 131.  Val. 2.4840560406446457, Acc. 0.5777777777777777\n",
      "Epoching: 132\n",
      "Epoch: 132.  Train. 0.45305185909232787, Acc. 0.7954430379746835\n",
      "Epoch: 132.  Val. 2.5120256394147873, Acc. 0.5777777777777777\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/132_model.pt\n",
      "Epoching: 133\n",
      "Epoch: 133.  Train. 0.44893127367381125, Acc. 0.7974683544303798\n",
      "Epoch: 133.  Val. 2.519589677453041, Acc. 0.5777777777777777\n",
      "Epoching: 134\n",
      "Epoch: 134.  Train. 0.4453425198312729, Acc. 0.7989873417721519\n",
      "Epoch: 134.  Val. 2.5384299606084824, Acc. 0.5733333333333334\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/134_model.pt\n",
      "Epoching: 135\n",
      "Epoch: 135.  Train. 0.4414541288729637, Acc. 0.8010126582278481\n",
      "Epoch: 135.  Val. 2.5416256487369537, Acc. 0.5777777777777777\n",
      "Epoching: 136\n",
      "Epoch: 136.  Train. 0.43770808366037184, Acc. 0.8045569620253165\n",
      "Epoch: 136.  Val. 2.568358913064003, Acc. 0.5777777777777777\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/136_model.pt\n",
      "Epoching: 137\n",
      "Epoch: 137.  Train. 0.43378327089932656, Acc. 0.8065822784810126\n",
      "Epoch: 137.  Val. 2.5817964673042297, Acc. 0.5777777777777777\n",
      "Epoching: 138\n",
      "Epoch: 138.  Train. 0.4299542884672842, Acc. 0.8086075949367089\n",
      "Epoch: 138.  Val. 2.5913227945566177, Acc. 0.5733333333333334\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/138_model.pt\n",
      "Epoching: 139\n",
      "Epoch: 139.  Train. 0.42636457109643566, Acc. 0.8091139240506329\n",
      "Epoch: 139.  Val. 2.614260032773018, Acc. 0.5733333333333334\n",
      "Epoching: 140\n",
      "Epoch: 140.  Train. 0.4221867649785934, Acc. 0.8116455696202531\n",
      "Epoch: 140.  Val. 2.61740805208683, Acc. 0.5733333333333334\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/140_model.pt\n",
      "Epoching: 141\n",
      "Epoch: 141.  Train. 0.418909200497212, Acc. 0.8131645569620253\n",
      "Epoch: 141.  Val. 2.6413333117961884, Acc. 0.5733333333333334\n",
      "Epoching: 142\n",
      "Epoch: 142.  Train. 0.4145309603021991, Acc. 0.8162025316455697\n",
      "Epoch: 142.  Val. 2.658795714378357, Acc. 0.5733333333333334\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/142_model.pt\n",
      "Epoching: 143\n",
      "Epoch: 143.  Train. 0.4112632445270015, Acc. 0.8167088607594937\n",
      "Epoch: 143.  Val. 2.657876491546631, Acc. 0.5733333333333334\n",
      "Epoching: 144\n",
      "Epoch: 144.  Train. 0.4071125921703154, Acc. 0.8187341772151899\n",
      "Epoch: 144.  Val. 2.69135519862175, Acc. 0.5733333333333334\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/144_model.pt\n",
      "Epoching: 145\n",
      "Epoch: 145.  Train. 0.4037050138558111, Acc. 0.8202531645569621\n",
      "Epoch: 145.  Val. 2.6744049042463303, Acc. 0.5733333333333334\n",
      "Epoching: 146\n",
      "Epoch: 146.  Train. 0.3996607601161926, Acc. 0.8217721518987342\n",
      "Epoch: 146.  Val. 2.717083364725113, Acc. 0.5733333333333334\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/146_model.pt\n",
      "Epoching: 147\n",
      "Epoch: 147.  Train. 0.3962129909665354, Acc. 0.8227848101265823\n",
      "Epoch: 147.  Val. 2.725087210536003, Acc. 0.5733333333333334\n",
      "Epoching: 148\n",
      "Epoch: 148.  Train. 0.39206384122371674, Acc. 0.8232911392405063\n",
      "Epoch: 148.  Val. 2.7267465740442276, Acc. 0.5733333333333334\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/148_model.pt\n",
      "Epoching: 149\n",
      "Epoch: 149.  Train. 0.3889665125358489, Acc. 0.8248101265822785\n",
      "Epoch: 149.  Val. 2.757440969347954, Acc. 0.5733333333333334\n",
      "Epoching: 150\n",
      "Epoch: 150.  Train. 0.384532725138049, Acc. 0.8258227848101266\n",
      "Epoch: 150.  Val. 2.7704508900642395, Acc. 0.5733333333333334\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/150_model.pt\n",
      "Epoching: 151\n",
      "Epoch: 151.  Train. 0.3814609569887961, Acc. 0.8283544303797469\n",
      "Epoch: 151.  Val. 2.766887068748474, Acc. 0.5688888888888889\n",
      "Epoching: 152\n",
      "Epoch: 152.  Train. 0.3775729537010193, Acc. 0.8313924050632912\n",
      "Epoch: 152.  Val. 2.8026461005210876, Acc. 0.5688888888888889\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/152_model.pt\n",
      "Epoching: 153\n",
      "Epoch: 153.  Train. 0.3740306792239989, Acc. 0.8334177215189873\n",
      "Epoch: 153.  Val. 2.790213018655777, Acc. 0.5688888888888889\n",
      "Epoching: 154\n",
      "Epoch: 154.  Train. 0.370285633110231, Acc. 0.8349367088607595\n",
      "Epoch: 154.  Val. 2.83071531355381, Acc. 0.5688888888888889\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/154_model.pt\n",
      "Epoching: 155\n",
      "Epoch: 155.  Train. 0.3668515057333054, Acc. 0.8364556962025317\n",
      "Epoch: 155.  Val. 2.8351705819368362, Acc. 0.5688888888888889\n",
      "Epoching: 156\n",
      "Epoch: 156.  Train. 0.3630296873950189, Acc. 0.8369620253164557\n",
      "Epoch: 156.  Val. 2.835241883993149, Acc. 0.5688888888888889\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/156_model.pt\n",
      "Epoching: 157\n",
      "Epoch: 157.  Train. 0.3597210496664047, Acc. 0.8384810126582278\n",
      "Epoch: 157.  Val. 2.87045980989933, Acc. 0.5688888888888889\n",
      "Epoching: 158\n",
      "Epoch: 158.  Train. 0.3560329572327675, Acc. 0.838987341772152\n",
      "Epoch: 158.  Val. 2.8834011405706406, Acc. 0.5688888888888889\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/158_model.pt\n",
      "Epoching: 159\n",
      "Epoch: 159.  Train. 0.3525266733861739, Acc. 0.8420253164556962\n",
      "Epoch: 159.  Val. 2.870868667960167, Acc. 0.5688888888888889\n",
      "Epoching: 160\n",
      "Epoch: 160.  Train. 0.34892172222175905, Acc. 0.8470886075949368\n",
      "Epoch: 160.  Val. 2.9204078167676926, Acc. 0.5688888888888889\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/160_model.pt\n",
      "Epoching: 161\n",
      "Epoch: 161.  Train. 0.3455481058166873, Acc. 0.8475949367088608\n",
      "Epoch: 161.  Val. 2.9266712367534637, Acc. 0.5688888888888889\n",
      "Epoching: 162\n",
      "Epoch: 162.  Train. 0.34197198479406293, Acc. 0.850126582278481\n",
      "Epoch: 162.  Val. 2.9263392984867096, Acc. 0.5688888888888889\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/162_model.pt\n",
      "Epoching: 163\n",
      "Epoch: 163.  Train. 0.33873665525067237, Acc. 0.8511392405063292\n",
      "Epoch: 163.  Val. 2.949993446469307, Acc. 0.5688888888888889\n",
      "Epoching: 164\n",
      "Epoch: 164.  Train. 0.33502940040442253, Acc. 0.8531645569620253\n",
      "Epoch: 164.  Val. 2.9510381668806076, Acc. 0.5688888888888889\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/164_model.pt\n",
      "Epoching: 165\n",
      "Epoch: 165.  Train. 0.3317983849875389, Acc. 0.8551898734177216\n",
      "Epoch: 165.  Val. 2.9793311208486557, Acc. 0.5733333333333334\n",
      "Epoching: 166\n",
      "Epoch: 166.  Train. 0.32832174003124237, Acc. 0.8556962025316456\n",
      "Epoch: 166.  Val. 2.9957403242588043, Acc. 0.5733333333333334\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/166_model.pt\n",
      "Epoching: 167\n",
      "Epoch: 167.  Train. 0.32495162948485345, Acc. 0.8577215189873417\n",
      "Epoch: 167.  Val. 2.9841208308935165, Acc. 0.5733333333333334\n",
      "Epoching: 168\n",
      "Epoch: 168.  Train. 0.3213256585501855, Acc. 0.860253164556962\n",
      "Epoch: 168.  Val. 3.037956491112709, Acc. 0.5733333333333334\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/168_model.pt\n",
      "Epoching: 169\n",
      "Epoch: 169.  Train. 0.3183403596762688, Acc. 0.8622784810126582\n",
      "Epoch: 169.  Val. 3.0209340304136276, Acc. 0.5733333333333334\n",
      "Epoching: 170\n",
      "Epoch: 170.  Train. 0.3148583166541592, Acc. 0.8648101265822785\n",
      "Epoch: 170.  Val. 3.049984186887741, Acc. 0.5688888888888889\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/170_model.pt\n",
      "Epoching: 171\n",
      "Epoch: 171.  Train. 0.3114701963720783, Acc. 0.8663291139240507\n",
      "Epoch: 171.  Val. 3.052754282951355, Acc. 0.5688888888888889\n",
      "Epoching: 172\n",
      "Epoch: 172.  Train. 0.30811525280437163, Acc. 0.8663291139240507\n",
      "Epoch: 172.  Val. 3.083169788122177, Acc. 0.5644444444444444\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/172_model.pt\n",
      "Epoching: 173\n",
      "Epoch: 173.  Train. 0.3045632817572163, Acc. 0.8673417721518988\n",
      "Epoch: 173.  Val. 3.072613775730133, Acc. 0.5688888888888889\n",
      "Epoching: 174\n",
      "Epoch: 174.  Train. 0.30146749341680157, Acc. 0.8703797468354431\n",
      "Epoch: 174.  Val. 3.1191518902778625, Acc. 0.5644444444444444\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/174_model.pt\n",
      "Epoching: 175\n",
      "Epoch: 175.  Train. 0.2986371603704268, Acc. 0.8703797468354431\n",
      "Epoch: 175.  Val. 3.1018985211849213, Acc. 0.5733333333333334\n",
      "Epoching: 176\n",
      "Epoch: 176.  Train. 0.2950363627845241, Acc. 0.8729113924050633\n",
      "Epoch: 176.  Val. 3.1472413539886475, Acc. 0.5644444444444444\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/176_model.pt\n",
      "Epoching: 177\n",
      "Epoch: 177.  Train. 0.2919295913269443, Acc. 0.8734177215189873\n",
      "Epoch: 177.  Val. 3.1153389513492584, Acc. 0.5688888888888889\n",
      "Epoching: 178\n",
      "Epoch: 178.  Train. 0.2881872471301786, Acc. 0.8744303797468355\n",
      "Epoch: 178.  Val. 3.177165240049362, Acc. 0.5688888888888889\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/178_model.pt\n",
      "Epoching: 179\n",
      "Epoch: 179.  Train. 0.28623048336275164, Acc. 0.8759493670886076\n",
      "Epoch: 179.  Val. 3.1741717755794525, Acc. 0.5688888888888889\n",
      "Epoching: 180\n",
      "Epoch: 180.  Train. 0.28301259226376013, Acc. 0.8769620253164557\n",
      "Epoch: 180.  Val. 3.197012275457382, Acc. 0.5688888888888889\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/180_model.pt\n",
      "Epoching: 181\n",
      "Epoch: 181.  Train. 0.27900295632500804, Acc. 0.8789873417721519\n",
      "Epoch: 181.  Val. 3.1829539239406586, Acc. 0.5644444444444444\n",
      "Epoching: 182\n",
      "Epoch: 182.  Train. 0.27614536977583365, Acc. 0.880506329113924\n",
      "Epoch: 182.  Val. 3.2275370657444, Acc. 0.56\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/182_model.pt\n",
      "Epoching: 183\n",
      "Epoch: 183.  Train. 0.27335676310523865, Acc. 0.8820253164556962\n",
      "Epoch: 183.  Val. 3.2057863771915436, Acc. 0.56\n",
      "Epoching: 184\n",
      "Epoch: 184.  Train. 0.2696965089248073, Acc. 0.8825316455696203\n",
      "Epoch: 184.  Val. 3.264027565717697, Acc. 0.56\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/184_model.pt\n",
      "Epoching: 185\n",
      "Epoch: 185.  Train. 0.2672786883288814, Acc. 0.8845569620253164\n",
      "Epoch: 185.  Val. 3.2505255937576294, Acc. 0.5555555555555556\n",
      "Epoching: 186\n",
      "Epoch: 186.  Train. 0.26338309698527856, Acc. 0.8881012658227848\n",
      "Epoch: 186.  Val. 3.269752651453018, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/186_model.pt\n",
      "Epoching: 187\n",
      "Epoch: 187.  Train. 0.2608039232511674, Acc. 0.889620253164557\n",
      "Epoch: 187.  Val. 3.2912199199199677, Acc. 0.5511111111111111\n",
      "Epoching: 188\n",
      "Epoch: 188.  Train. 0.2581593031364103, Acc. 0.8906329113924051\n",
      "Epoch: 188.  Val. 3.284178853034973, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/188_model.pt\n",
      "Epoching: 189\n",
      "Epoch: 189.  Train. 0.2544693374826062, Acc. 0.8916455696202532\n",
      "Epoch: 189.  Val. 3.3286200761795044, Acc. 0.5555555555555556\n",
      "Epoching: 190\n",
      "Epoch: 190.  Train. 0.2525652380239579, Acc. 0.8921518987341772\n",
      "Epoch: 190.  Val. 3.3417994379997253, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/190_model.pt\n",
      "Epoching: 191\n",
      "Epoch: 191.  Train. 0.24846864744059502, Acc. 0.8956962025316456\n",
      "Epoch: 191.  Val. 3.3204660415649414, Acc. 0.56\n",
      "Epoching: 192\n",
      "Epoch: 192.  Train. 0.24601066208654834, Acc. 0.8962025316455696\n",
      "Epoch: 192.  Val. 3.3782931864261627, Acc. 0.5644444444444444\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/192_model.pt\n",
      "Epoching: 193\n",
      "Epoch: 193.  Train. 0.24416355548366422, Acc. 0.8962025316455696\n",
      "Epoch: 193.  Val. 3.3738146126270294, Acc. 0.5644444444444444\n",
      "Epoching: 194\n",
      "Epoch: 194.  Train. 0.23998560660308407, Acc. 0.8982278481012658\n",
      "Epoch: 194.  Val. 3.374378055334091, Acc. 0.5644444444444444\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/194_model.pt\n",
      "Epoching: 195\n",
      "Epoch: 195.  Train. 0.2374936919058523, Acc. 0.900759493670886\n",
      "Epoch: 195.  Val. 3.411061316728592, Acc. 0.56\n",
      "Epoching: 196\n",
      "Epoch: 196.  Train. 0.2349196603942302, Acc. 0.9027848101265823\n",
      "Epoch: 196.  Val. 3.430199772119522, Acc. 0.56\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/196_model.pt\n",
      "Epoching: 197\n",
      "Epoch: 197.  Train. 0.2321358889101013, Acc. 0.9032911392405063\n",
      "Epoch: 197.  Val. 3.401010185480118, Acc. 0.5644444444444444\n",
      "Epoching: 198\n",
      "Epoch: 198.  Train. 0.22766197761220316, Acc. 0.9058227848101266\n",
      "Epoch: 198.  Val. 3.473531663417816, Acc. 0.56\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/198_model.pt\n",
      "Epoching: 199\n",
      "Epoch: 199.  Train. 0.22738698593551113, Acc. 0.9063291139240506\n",
      "Epoch: 199.  Val. 3.4603399634361267, Acc. 0.56\n",
      "Epoching: 200\n",
      "Epoch: 200.  Train. 0.22428506276299875, Acc. 0.9068354430379747\n",
      "Epoch: 200.  Val. 3.460143059492111, Acc. 0.5644444444444444\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/200_model.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_freq = 2\n",
    "\n",
    "for epoch in range(101,201):\n",
    "    epoch_loss = 0\n",
    "    val_loss = 0\n",
    "    preds, gt = [], []\n",
    "    print('Epoching:', epoch)\n",
    "    \n",
    "    model.train() \n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = data\n",
    "        #print(inputs, labels)\n",
    "        inputs = inputs.to(params['device'], non_blocking=True)\n",
    "        labels = labels.to(params['device'], non_blocking=True)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        #print(loss.item())\n",
    "        \n",
    "        proba = F.softmax(outputs, dim=1)\n",
    "        pred = torch.argmax(proba, dim=1)\n",
    "        \n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        \n",
    "        preds.extend(list(pred))\n",
    "        gt.extend(list(labels))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    acc = accuracy_score(preds, gt)\n",
    "    #print(gt)\n",
    "    avg_loss_train = epoch_loss / len(train_loader)\n",
    "    hist_train_loss.append(avg_loss_train)\n",
    "    print( \"Epoch: {epoch}.  Train. {metric_monitor}, Acc. {accuracy}\".format(epoch=epoch, \n",
    "                                                                            metric_monitor=avg_loss_train,\n",
    "                                                                            accuracy = acc))\n",
    "    preds, gt = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            optimizer.zero_grad()\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(params['device'], non_blocking=True)\n",
    "            labels = labels.to(params['device'], non_blocking=True)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "        \n",
    "            proba = F.softmax(outputs, dim=1)\n",
    "            pred = torch.argmax(proba, dim=1)\n",
    "        \n",
    "            pred = pred.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "        \n",
    "            preds.extend(list(pred))\n",
    "            gt.extend(list(labels))\n",
    "        \n",
    "    \n",
    "        acc = accuracy_score(preds, gt)\n",
    "        avg_loss_val = val_loss / len(val_loader)\n",
    "        hist_val_loss.append(avg_loss_val)\n",
    "        print( \"Epoch: {epoch}.  Val. {metric_monitor}, Acc. {accuracy}\".format(epoch=epoch, \n",
    "                                                                            metric_monitor=avg_loss_val,\n",
    "                                                                            accuracy = acc))\n",
    "    \n",
    "\n",
    "    if epoch % save_freq == 0:\n",
    "        torch.save(model.state_dict(),os.path.join(params['model_save_path'],str(epoch)+\"_model.pt\"))\n",
    "        print(\"Saved model to {}\".format(os.path.join(params['model_save_path'], str(epoch)+\"_model.pt\")))\n",
    "    #print(best_loss_val)\n",
    "    if hist_val_loss[-1] < best_loss_val:\n",
    "        torch.save(model.state_dict(),os.path.join(params['model_save_path'],\"model_best.pt\"))\n",
    "        print(\"Saved model to {}\".format(os.path.join(params['model_save_path'], \"model_best.pt\")))\n",
    "        best_loss_val = hist_val_loss[-1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2256ef06-4471-4d24-bf98-c6a68d47cc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at owkin/phikon were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "save = os.path.join(params['model_save_path'])\n",
    "model=MyModel(768,5)\n",
    "model = model.to(params[\"device\"])\n",
    "model.load_state_dict(torch.load(os.path.join(save, \"200_model.pt\")))\n",
    "weights = torch.tensor([0.0607, 0.0184, 0.0293, 0.8695, 0.0221]).to(params['device'])\n",
    "criterion = nn.CrossEntropyLoss(weight=weights).to(params['device'])\n",
    "optimizer = optim.AdamW(model.parameters(), \n",
    "                            lr=params['lr'], \n",
    "                            weight_decay=params[\"weight_decay\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "68f53107-e3b6-442b-abcc-843d014bf5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoching: 201\n",
      "Epoch: 201.  Train. 0.2977245294248625, Acc. 0.8648101265822785\n",
      "Epoch: 201.  Val. 3.744849056005478, Acc. 0.48\n",
      "Epoching: 202\n",
      "Epoch: 202.  Train. 0.3227869827420481, Acc. 0.859746835443038\n",
      "Epoch: 202.  Val. 3.372103065252304, Acc. 0.5688888888888889\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/202_model.pt\n",
      "Epoching: 203\n",
      "Epoch: 203.  Train. 0.2251427353870484, Acc. 0.899746835443038\n",
      "Epoch: 203.  Val. 3.516812890768051, Acc. 0.56\n",
      "Epoching: 204\n",
      "Epoch: 204.  Train. 0.2125483498938622, Acc. 0.9048101265822784\n",
      "Epoch: 204.  Val. 3.5507465600967407, Acc. 0.56\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/204_model.pt\n",
      "Epoching: 205\n",
      "Epoch: 205.  Train. 0.2115632913285686, Acc. 0.9083544303797468\n",
      "Epoch: 205.  Val. 3.545643448829651, Acc. 0.5555555555555556\n",
      "Epoching: 206\n",
      "Epoch: 206.  Train. 0.20771900059715395, Acc. 0.9129113924050632\n",
      "Epoch: 206.  Val. 3.575462192296982, Acc. 0.56\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/206_model.pt\n",
      "Epoching: 207\n",
      "Epoch: 207.  Train. 0.20511997719445535, Acc. 0.9139240506329114\n",
      "Epoch: 207.  Val. 3.597573906183243, Acc. 0.56\n",
      "Epoching: 208\n",
      "Epoch: 208.  Train. 0.20433424917920942, Acc. 0.9164556962025316\n",
      "Epoch: 208.  Val. 3.616810530424118, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/208_model.pt\n",
      "Epoching: 209\n",
      "Epoch: 209.  Train. 0.2029992172554616, Acc. 0.9179746835443038\n",
      "Epoch: 209.  Val. 3.642101228237152, Acc. 0.56\n",
      "Epoching: 210\n",
      "Epoch: 210.  Train. 0.2018187905271207, Acc. 0.9174683544303798\n",
      "Epoch: 210.  Val. 3.658872216939926, Acc. 0.56\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/210_model.pt\n",
      "Epoching: 211\n",
      "Epoch: 211.  Train. 0.20053752559807994, Acc. 0.9179746835443038\n",
      "Epoch: 211.  Val. 3.690573990345001, Acc. 0.56\n",
      "Epoching: 212\n",
      "Epoch: 212.  Train. 0.19867268481081532, Acc. 0.9179746835443038\n",
      "Epoch: 212.  Val. 3.7087590098381042, Acc. 0.56\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/212_model.pt\n",
      "Epoching: 213\n",
      "Epoch: 213.  Train. 0.19621623211329983, Acc. 0.9194936708860759\n",
      "Epoch: 213.  Val. 3.743812143802643, Acc. 0.5555555555555556\n",
      "Epoching: 214\n",
      "Epoch: 214.  Train. 0.19367099204851734, Acc. 0.9215189873417722\n",
      "Epoch: 214.  Val. 3.7657464146614075, Acc. 0.56\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/214_model.pt\n",
      "Epoching: 215\n",
      "Epoch: 215.  Train. 0.19097787249953516, Acc. 0.9225316455696203\n",
      "Epoch: 215.  Val. 3.7896725833415985, Acc. 0.56\n",
      "Epoching: 216\n",
      "Epoch: 216.  Train. 0.18823000860791053, Acc. 0.9235443037974683\n",
      "Epoch: 216.  Val. 3.817541927099228, Acc. 0.5644444444444444\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/216_model.pt\n",
      "Epoching: 217\n",
      "Epoch: 217.  Train. 0.18569284425147117, Acc. 0.9240506329113924\n",
      "Epoch: 217.  Val. 3.842252016067505, Acc. 0.5644444444444444\n",
      "Epoching: 218\n",
      "Epoch: 218.  Train. 0.18304021392137773, Acc. 0.9250632911392405\n",
      "Epoch: 218.  Val. 3.8636544346809387, Acc. 0.56\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/218_model.pt\n",
      "Epoching: 219\n",
      "Epoch: 219.  Train. 0.18042159981785283, Acc. 0.9275949367088607\n",
      "Epoch: 219.  Val. 3.886692851781845, Acc. 0.56\n",
      "Epoching: 220\n",
      "Epoch: 220.  Train. 0.17791617866004666, Acc. 0.9281012658227848\n",
      "Epoch: 220.  Val. 3.908820688724518, Acc. 0.56\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/220_model.pt\n",
      "Epoching: 221\n",
      "Epoch: 221.  Train. 0.17535114744978567, Acc. 0.9291139240506329\n",
      "Epoch: 221.  Val. 3.9358915388584137, Acc. 0.56\n",
      "Epoching: 222\n",
      "Epoch: 222.  Train. 0.1729260375422816, Acc. 0.930126582278481\n",
      "Epoch: 222.  Val. 3.95417720079422, Acc. 0.56\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/222_model.pt\n",
      "Epoching: 223\n",
      "Epoch: 223.  Train. 0.17036548028549842, Acc. 0.9306329113924051\n",
      "Epoch: 223.  Val. 3.9797679781913757, Acc. 0.5555555555555556\n",
      "Epoching: 224\n",
      "Epoch: 224.  Train. 0.1679877637615127, Acc. 0.9321518987341773\n",
      "Epoch: 224.  Val. 4.000316351652145, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/224_model.pt\n",
      "Epoching: 225\n",
      "Epoch: 225.  Train. 0.16540722512910444, Acc. 0.9326582278481013\n",
      "Epoch: 225.  Val. 4.020999729633331, Acc. 0.5555555555555556\n",
      "Epoching: 226\n",
      "Epoch: 226.  Train. 0.1631451837958828, Acc. 0.9346835443037975\n",
      "Epoch: 226.  Val. 4.037838727235794, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/226_model.pt\n",
      "Epoching: 227\n",
      "Epoch: 227.  Train. 0.1604446994681512, Acc. 0.9351898734177215\n",
      "Epoch: 227.  Val. 4.064244598150253, Acc. 0.5555555555555556\n",
      "Epoching: 228\n",
      "Epoch: 228.  Train. 0.15838365773520163, Acc. 0.9356962025316455\n",
      "Epoch: 228.  Val. 4.084404289722443, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/228_model.pt\n",
      "Epoching: 229\n",
      "Epoch: 229.  Train. 0.15559343092383876, Acc. 0.9362025316455697\n",
      "Epoch: 229.  Val. 4.111863225698471, Acc. 0.5555555555555556\n",
      "Epoching: 230\n",
      "Epoch: 230.  Train. 0.15375004768852266, Acc. 0.9372151898734177\n",
      "Epoch: 230.  Val. 4.125921964645386, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/230_model.pt\n",
      "Epoching: 231\n",
      "Epoch: 231.  Train. 0.15075252169082243, Acc. 0.9397468354430379\n",
      "Epoch: 231.  Val. 4.14922109246254, Acc. 0.5555555555555556\n",
      "Epoching: 232\n",
      "Epoch: 232.  Train. 0.14912169102218845, Acc. 0.9412658227848101\n",
      "Epoch: 232.  Val. 4.171706736087799, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/232_model.pt\n",
      "Epoching: 233\n",
      "Epoch: 233.  Train. 0.1461073757419663, Acc. 0.9432911392405063\n",
      "Epoch: 233.  Val. 4.187485158443451, Acc. 0.5555555555555556\n",
      "Epoching: 234\n",
      "Epoch: 234.  Train. 0.14458686245545263, Acc. 0.9443037974683545\n",
      "Epoch: 234.  Val. 4.206277549266815, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/234_model.pt\n",
      "Epoching: 235\n",
      "Epoch: 235.  Train. 0.14160228712904838, Acc. 0.9448101265822785\n",
      "Epoch: 235.  Val. 4.234830260276794, Acc. 0.5555555555555556\n",
      "Epoching: 236\n",
      "Epoch: 236.  Train. 0.1401072011118935, Acc. 0.9473417721518987\n",
      "Epoch: 236.  Val. 4.246705114841461, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/236_model.pt\n",
      "Epoching: 237\n",
      "Epoch: 237.  Train. 0.1371791820612646, Acc. 0.9478481012658228\n",
      "Epoch: 237.  Val. 4.275511622428894, Acc. 0.5555555555555556\n",
      "Epoching: 238\n",
      "Epoch: 238.  Train. 0.13578235406068065, Acc. 0.9478481012658228\n",
      "Epoch: 238.  Val. 4.287235826253891, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/238_model.pt\n",
      "Epoching: 239\n",
      "Epoch: 239.  Train. 0.1328544454468835, Acc. 0.949873417721519\n",
      "Epoch: 239.  Val. 4.31565660238266, Acc. 0.5555555555555556\n",
      "Epoching: 240\n",
      "Epoch: 240.  Train. 0.13146243040119449, Acc. 0.949873417721519\n",
      "Epoch: 240.  Val. 4.328283429145813, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/240_model.pt\n",
      "Epoching: 241\n",
      "Epoch: 241.  Train. 0.12863187491893768, Acc. 0.950379746835443\n",
      "Epoch: 241.  Val. 4.355050623416901, Acc. 0.5555555555555556\n",
      "Epoching: 242\n",
      "Epoch: 242.  Train. 0.1262184190413644, Acc. 0.9513924050632911\n",
      "Epoch: 242.  Val. 4.364198923110962, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/242_model.pt\n",
      "Epoching: 243\n",
      "Epoch: 243.  Train. 0.12512435734031663, Acc. 0.9529113924050633\n",
      "Epoch: 243.  Val. 4.3944752514362335, Acc. 0.5555555555555556\n",
      "Epoching: 244\n",
      "Epoch: 244.  Train. 0.12291486417093585, Acc. 0.9534177215189873\n",
      "Epoch: 244.  Val. 4.401239991188049, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/244_model.pt\n",
      "Epoching: 245\n",
      "Epoch: 245.  Train. 0.12056814692914486, Acc. 0.9544303797468354\n",
      "Epoch: 245.  Val. 4.431131839752197, Acc. 0.56\n",
      "Epoching: 246\n",
      "Epoch: 246.  Train. 0.1193184949938328, Acc. 0.9559493670886076\n",
      "Epoch: 246.  Val. 4.44549885392189, Acc. 0.56\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/246_model.pt\n",
      "Epoching: 247\n",
      "Epoch: 247.  Train. 0.11656115342292094, Acc. 0.9569620253164557\n",
      "Epoch: 247.  Val. 4.47420397400856, Acc. 0.56\n",
      "Epoching: 248\n",
      "Epoch: 248.  Train. 0.1155801611922441, Acc. 0.9574683544303797\n",
      "Epoch: 248.  Val. 4.48574635386467, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/248_model.pt\n",
      "Epoching: 249\n",
      "Epoch: 249.  Train. 0.1127908993512392, Acc. 0.9579746835443038\n",
      "Epoch: 249.  Val. 4.51149645447731, Acc. 0.56\n",
      "Epoching: 250\n",
      "Epoch: 250.  Train. 0.11191998860768733, Acc. 0.958987341772152\n",
      "Epoch: 250.  Val. 4.5229297280311584, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/250_model.pt\n"
     ]
    }
   ],
   "source": [
    "save_freq = 2\n",
    "\n",
    "for epoch in range(201,251):\n",
    "    epoch_loss = 0\n",
    "    val_loss = 0\n",
    "    preds, gt = [], []\n",
    "    print('Epoching:', epoch)\n",
    "    \n",
    "    model.train() \n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = data\n",
    "        #print(inputs, labels)\n",
    "        inputs = inputs.to(params['device'], non_blocking=True)\n",
    "        labels = labels.to(params['device'], non_blocking=True)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        #print(loss.item())\n",
    "        \n",
    "        proba = F.softmax(outputs, dim=1)\n",
    "        pred = torch.argmax(proba, dim=1)\n",
    "        \n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        \n",
    "        preds.extend(list(pred))\n",
    "        gt.extend(list(labels))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    acc = accuracy_score(preds, gt)\n",
    "    #print(gt)\n",
    "    avg_loss_train = epoch_loss / len(train_loader)\n",
    "    hist_train_loss.append(avg_loss_train)\n",
    "    print( \"Epoch: {epoch}.  Train. {metric_monitor}, Acc. {accuracy}\".format(epoch=epoch, \n",
    "                                                                            metric_monitor=avg_loss_train,\n",
    "                                                                            accuracy = acc))\n",
    "    preds, gt = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            optimizer.zero_grad()\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(params['device'], non_blocking=True)\n",
    "            labels = labels.to(params['device'], non_blocking=True)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "        \n",
    "            proba = F.softmax(outputs, dim=1)\n",
    "            pred = torch.argmax(proba, dim=1)\n",
    "        \n",
    "            pred = pred.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "        \n",
    "            preds.extend(list(pred))\n",
    "            gt.extend(list(labels))\n",
    "        \n",
    "    \n",
    "        acc = accuracy_score(preds, gt)\n",
    "        avg_loss_val = val_loss / len(val_loader)\n",
    "        hist_val_loss.append(avg_loss_val)\n",
    "        print( \"Epoch: {epoch}.  Val. {metric_monitor}, Acc. {accuracy}\".format(epoch=epoch, \n",
    "                                                                            metric_monitor=avg_loss_val,\n",
    "                                                                            accuracy = acc))\n",
    "    \n",
    "\n",
    "    if epoch % save_freq == 0:\n",
    "        torch.save(model.state_dict(),os.path.join(params['model_save_path'],str(epoch)+\"_model.pt\"))\n",
    "        print(\"Saved model to {}\".format(os.path.join(params['model_save_path'], str(epoch)+\"_model.pt\")))\n",
    "    #print(best_loss_val)\n",
    "    if hist_val_loss[-1] < best_loss_val:\n",
    "        torch.save(model.state_dict(),os.path.join(params['model_save_path'],\"model_best.pt\"))\n",
    "        print(\"Saved model to {}\".format(os.path.join(params['model_save_path'], \"model_best.pt\")))\n",
    "        best_loss_val = hist_val_loss[-1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "45f37bc0-a476-4824-a150-615669ed171b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at owkin/phikon were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "save = os.path.join(params['model_save_path'])\n",
    "model=MyModel(768,5)\n",
    "model = model.to(params[\"device\"])\n",
    "model.load_state_dict(torch.load(os.path.join(save, \"250_model.pt\")))\n",
    "weights = torch.tensor([0.0607, 0.0184, 0.0293, 0.8695, 0.0221]).to(params['device'])\n",
    "criterion = nn.CrossEntropyLoss(weight=weights).to(params['device'])\n",
    "optimizer = optim.AdamW(model.parameters(), \n",
    "                            lr=params['lr'], \n",
    "                            weight_decay=params[\"weight_decay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9dbfe3bd-55f8-41a0-bf4c-0c2ca64f3152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoching: 251\n",
      "Epoch: 251.  Train. 0.18254636889398698, Acc. 0.9230379746835443\n",
      "Epoch: 251.  Val. 4.796628504991531, Acc. 0.47555555555555556\n",
      "Epoching: 252\n",
      "Epoch: 252.  Train. 0.1734321338515128, Acc. 0.9311392405063291\n",
      "Epoch: 252.  Val. 4.327044725418091, Acc. 0.56\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/252_model.pt\n",
      "Epoching: 253\n",
      "Epoch: 253.  Train. 0.1244018622223408, Acc. 0.9478481012658228\n",
      "Epoch: 253.  Val. 4.519078254699707, Acc. 0.56\n",
      "Epoching: 254\n",
      "Epoch: 254.  Train. 0.1260019192592271, Acc. 0.9473417721518987\n",
      "Epoch: 254.  Val. 4.526518642902374, Acc. 0.56\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/254_model.pt\n",
      "Epoching: 255\n",
      "Epoch: 255.  Train. 0.12635823092874018, Acc. 0.9443037974683545\n",
      "Epoch: 255.  Val. 4.518211454153061, Acc. 0.56\n",
      "Epoching: 256\n",
      "Epoch: 256.  Train. 0.120955609806603, Acc. 0.9513924050632911\n",
      "Epoch: 256.  Val. 4.543442904949188, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/256_model.pt\n",
      "Epoching: 257\n",
      "Epoch: 257.  Train. 0.11212304372700953, Acc. 0.958987341772152\n",
      "Epoch: 257.  Val. 4.588738977909088, Acc. 0.5555555555555556\n",
      "Epoching: 258\n",
      "Epoch: 258.  Train. 0.10524104331289569, Acc. 0.9625316455696202\n",
      "Epoch: 258.  Val. 4.622594952583313, Acc. 0.5511111111111111\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/258_model.pt\n",
      "Epoching: 259\n",
      "Epoch: 259.  Train. 0.1022420700399145, Acc. 0.9681012658227848\n",
      "Epoch: 259.  Val. 4.646973937749863, Acc. 0.5511111111111111\n",
      "Epoching: 260\n",
      "Epoch: 260.  Train. 0.102046252018021, Acc. 0.9670886075949368\n",
      "Epoch: 260.  Val. 4.657320410013199, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/260_model.pt\n",
      "Epoching: 261\n",
      "Epoch: 261.  Train. 0.10295772462362243, Acc. 0.9650632911392405\n",
      "Epoch: 261.  Val. 4.659550905227661, Acc. 0.56\n",
      "Epoching: 262\n",
      "Epoch: 262.  Train. 0.10379120941844679, Acc. 0.9660759493670886\n",
      "Epoch: 262.  Val. 4.669645845890045, Acc. 0.56\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/262_model.pt\n",
      "Epoching: 263\n",
      "Epoch: 263.  Train. 0.10329913672420286, Acc. 0.9655696202531645\n",
      "Epoch: 263.  Val. 4.6855998039245605, Acc. 0.5555555555555556\n",
      "Epoching: 264\n",
      "Epoch: 264.  Train. 0.10190333317845099, Acc. 0.9665822784810126\n",
      "Epoch: 264.  Val. 4.708787888288498, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/264_model.pt\n",
      "Epoching: 265\n",
      "Epoch: 265.  Train. 0.10039996722292516, Acc. 0.9675949367088608\n",
      "Epoch: 265.  Val. 4.749071091413498, Acc. 0.5555555555555556\n",
      "Epoching: 266\n",
      "Epoch: 266.  Train. 0.0980146812094796, Acc. 0.9670886075949368\n",
      "Epoch: 266.  Val. 4.774354845285416, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/266_model.pt\n",
      "Epoching: 267\n",
      "Epoch: 267.  Train. 0.09632211809437122, Acc. 0.9686075949367089\n",
      "Epoch: 267.  Val. 4.811138570308685, Acc. 0.5555555555555556\n",
      "Epoching: 268\n",
      "Epoch: 268.  Train. 0.09520861133933067, Acc. 0.9686075949367089\n",
      "Epoch: 268.  Val. 4.8439745008945465, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/268_model.pt\n",
      "Epoching: 269\n",
      "Epoch: 269.  Train. 0.09375892303163005, Acc. 0.9691139240506329\n",
      "Epoch: 269.  Val. 4.879416078329086, Acc. 0.5555555555555556\n",
      "Epoching: 270\n",
      "Epoch: 270.  Train. 0.09271523332403551, Acc. 0.9681012658227848\n",
      "Epoch: 270.  Val. 4.911177784204483, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/270_model.pt\n",
      "Epoching: 271\n",
      "Epoch: 271.  Train. 0.09255634518640657, Acc. 0.9686075949367089\n",
      "Epoch: 271.  Val. 4.9424620270729065, Acc. 0.5555555555555556\n",
      "Epoching: 272\n",
      "Epoch: 272.  Train. 0.0909592256310486, Acc. 0.9696202531645569\n",
      "Epoch: 272.  Val. 4.979585736989975, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/272_model.pt\n",
      "Epoching: 273\n",
      "Epoch: 273.  Train. 0.09070823101266738, Acc. 0.970632911392405\n",
      "Epoch: 273.  Val. 5.007556974887848, Acc. 0.5555555555555556\n",
      "Epoching: 274\n",
      "Epoch: 274.  Train. 0.09004714640398179, Acc. 0.970632911392405\n",
      "Epoch: 274.  Val. 5.039751648902893, Acc. 0.5511111111111111\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/274_model.pt\n",
      "Epoching: 275\n",
      "Epoch: 275.  Train. 0.08925443515181541, Acc. 0.970126582278481\n",
      "Epoch: 275.  Val. 5.077498435974121, Acc. 0.5511111111111111\n",
      "Epoching: 276\n",
      "Epoch: 276.  Train. 0.08821508811125832, Acc. 0.9721518987341772\n",
      "Epoch: 276.  Val. 5.095364362001419, Acc. 0.5511111111111111\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/276_model.pt\n",
      "Epoching: 277\n",
      "Epoch: 277.  Train. 0.08882041257475654, Acc. 0.9721518987341772\n",
      "Epoch: 277.  Val. 5.116210371255875, Acc. 0.5511111111111111\n",
      "Epoching: 278\n",
      "Epoch: 278.  Train. 0.08630266328973155, Acc. 0.9716455696202532\n",
      "Epoch: 278.  Val. 5.143380403518677, Acc. 0.5511111111111111\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/278_model.pt\n",
      "Epoching: 279\n",
      "Epoch: 279.  Train. 0.0884886436885403, Acc. 0.9721518987341772\n",
      "Epoch: 279.  Val. 5.182362675666809, Acc. 0.5511111111111111\n",
      "Epoching: 280\n",
      "Epoch: 280.  Train. 0.08473815300291584, Acc. 0.9726582278481013\n",
      "Epoch: 280.  Val. 5.191057533025742, Acc. 0.5511111111111111\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/280_model.pt\n",
      "Epoching: 281\n",
      "Epoch: 281.  Train. 0.08888329349217876, Acc. 0.9711392405063292\n",
      "Epoch: 281.  Val. 5.229583382606506, Acc. 0.5466666666666666\n",
      "Epoching: 282\n",
      "Epoch: 282.  Train. 0.08310456513877838, Acc. 0.9741772151898734\n",
      "Epoch: 282.  Val. 5.23486801981926, Acc. 0.5511111111111111\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/282_model.pt\n",
      "Epoching: 283\n",
      "Epoch: 283.  Train. 0.09012985992575845, Acc. 0.970632911392405\n",
      "Epoch: 283.  Val. 5.277596980333328, Acc. 0.5422222222222223\n",
      "Epoching: 284\n",
      "Epoch: 284.  Train. 0.08168921457423318, Acc. 0.9751898734177216\n",
      "Epoch: 284.  Val. 5.276144832372665, Acc. 0.5511111111111111\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/284_model.pt\n",
      "Epoching: 285\n",
      "Epoch: 285.  Train. 0.09236079256140417, Acc. 0.9711392405063292\n",
      "Epoch: 285.  Val. 5.317906022071838, Acc. 0.5422222222222223\n",
      "Epoching: 286\n",
      "Epoch: 286.  Train. 0.08069832130305228, Acc. 0.9731645569620253\n",
      "Epoch: 286.  Val. 5.31546476483345, Acc. 0.5511111111111111\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/286_model.pt\n",
      "Epoching: 287\n",
      "Epoch: 287.  Train. 0.09516058069082999, Acc. 0.970632911392405\n",
      "Epoch: 287.  Val. 5.3473760187625885, Acc. 0.5422222222222223\n",
      "Epoching: 288\n",
      "Epoch: 288.  Train. 0.08033583373312027, Acc. 0.9736708860759493\n",
      "Epoch: 288.  Val. 5.342848390340805, Acc. 0.5511111111111111\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/288_model.pt\n",
      "Epoching: 289\n",
      "Epoch: 289.  Train. 0.09833396851055083, Acc. 0.970126582278481\n",
      "Epoch: 289.  Val. 5.375082284212112, Acc. 0.5377777777777778\n",
      "Epoching: 290\n",
      "Epoch: 290.  Train. 0.08104651271095199, Acc. 0.9726582278481013\n",
      "Epoch: 290.  Val. 5.373607933521271, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/290_model.pt\n",
      "Epoching: 291\n",
      "Epoch: 291.  Train. 0.10130832659741563, Acc. 0.9681012658227848\n",
      "Epoch: 291.  Val. 5.40189990401268, Acc. 0.5422222222222223\n",
      "Epoching: 292\n",
      "Epoch: 292.  Train. 0.08381673958032362, Acc. 0.9731645569620253\n",
      "Epoch: 292.  Val. 5.429083466529846, Acc. 0.5511111111111111\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/292_model.pt\n",
      "Epoching: 293\n",
      "Epoch: 293.  Train. 0.10387704710686399, Acc. 0.9691139240506329\n",
      "Epoch: 293.  Val. 5.445521920919418, Acc. 0.5422222222222223\n",
      "Epoching: 294\n",
      "Epoch: 294.  Train. 0.08862009127774546, Acc. 0.970632911392405\n",
      "Epoch: 294.  Val. 5.462998956441879, Acc. 0.5511111111111111\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/294_model.pt\n",
      "Epoching: 295\n",
      "Epoch: 295.  Train. 0.10614076280786146, Acc. 0.9675949367088608\n",
      "Epoch: 295.  Val. 5.427403122186661, Acc. 0.5377777777777778\n",
      "Epoching: 296\n",
      "Epoch: 296.  Train. 0.09396537585604575, Acc. 0.9675949367088608\n",
      "Epoch: 296.  Val. 5.458259731531143, Acc. 0.5555555555555556\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/296_model.pt\n",
      "Epoching: 297\n",
      "Epoch: 297.  Train. 0.10443082367700915, Acc. 0.9635443037974684\n",
      "Epoch: 297.  Val. 5.322811096906662, Acc. 0.5288888888888889\n",
      "Epoching: 298\n",
      "Epoch: 298.  Train. 0.09920012536308458, Acc. 0.9645569620253165\n",
      "Epoch: 298.  Val. 5.420708268880844, Acc. 0.5466666666666666\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/298_model.pt\n",
      "Epoching: 299\n",
      "Epoch: 299.  Train. 0.10366891567865687, Acc. 0.9660759493670886\n",
      "Epoch: 299.  Val. 5.210592299699783, Acc. 0.5377777777777778\n",
      "Epoching: 300\n",
      "Epoch: 300.  Train. 0.101928377103421, Acc. 0.9625316455696202\n",
      "Epoch: 300.  Val. 5.2843159437179565, Acc. 0.5511111111111111\n",
      "Saved model to ./models/1_grade_classification_in_NLST_phikon_224/300_model.pt\n"
     ]
    }
   ],
   "source": [
    "save_freq = 2\n",
    "\n",
    "for epoch in range(251,301):\n",
    "    epoch_loss = 0\n",
    "    val_loss = 0\n",
    "    preds, gt = [], []\n",
    "    print('Epoching:', epoch)\n",
    "    \n",
    "    model.train() \n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = data\n",
    "        #print(inputs, labels)\n",
    "        inputs = inputs.to(params['device'], non_blocking=True)\n",
    "        labels = labels.to(params['device'], non_blocking=True)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        #print(loss.item())\n",
    "        \n",
    "        proba = F.softmax(outputs, dim=1)\n",
    "        pred = torch.argmax(proba, dim=1)\n",
    "        \n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        \n",
    "        preds.extend(list(pred))\n",
    "        gt.extend(list(labels))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    acc = accuracy_score(preds, gt)\n",
    "    #print(gt)\n",
    "    avg_loss_train = epoch_loss / len(train_loader)\n",
    "    hist_train_loss.append(avg_loss_train)\n",
    "    print( \"Epoch: {epoch}.  Train. {metric_monitor}, Acc. {accuracy}\".format(epoch=epoch, \n",
    "                                                                            metric_monitor=avg_loss_train,\n",
    "                                                                            accuracy = acc))\n",
    "    preds, gt = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            optimizer.zero_grad()\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(params['device'], non_blocking=True)\n",
    "            labels = labels.to(params['device'], non_blocking=True)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "        \n",
    "            proba = F.softmax(outputs, dim=1)\n",
    "            pred = torch.argmax(proba, dim=1)\n",
    "        \n",
    "            pred = pred.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "        \n",
    "            preds.extend(list(pred))\n",
    "            gt.extend(list(labels))\n",
    "        \n",
    "    \n",
    "        acc = accuracy_score(preds, gt)\n",
    "        avg_loss_val = val_loss / len(val_loader)\n",
    "        hist_val_loss.append(avg_loss_val)\n",
    "        print( \"Epoch: {epoch}.  Val. {metric_monitor}, Acc. {accuracy}\".format(epoch=epoch, \n",
    "                                                                            metric_monitor=avg_loss_val,\n",
    "                                                                            accuracy = acc))\n",
    "    \n",
    "\n",
    "    if epoch % save_freq == 0:\n",
    "        torch.save(model.state_dict(),os.path.join(params['model_save_path'],str(epoch)+\"_model.pt\"))\n",
    "        print(\"Saved model to {}\".format(os.path.join(params['model_save_path'], str(epoch)+\"_model.pt\")))\n",
    "    #print(best_loss_val)\n",
    "    if hist_val_loss[-1] < best_loss_val:\n",
    "        torch.save(model.state_dict(),os.path.join(params['model_save_path'],\"model_best.pt\"))\n",
    "        print(\"Saved model to {}\".format(os.path.join(params['model_save_path'], \"model_best.pt\")))\n",
    "        best_loss_val = hist_val_loss[-1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cdcd73f-b171-497e-a457-3ff2c95b7bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at owkin/phikon were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[242  94  68   0  21]\n",
      " [ 34 756 366   2  32]\n",
      " [  9  49 583   0  14]\n",
      " [  2   5  22  15   1]\n",
      " [  9  23  69   0 909]]\n",
      "0.7533834586466165\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_df = pd.read_csv(params['test_dataset'])\n",
    "test_dataset = ImageDataset(test_df, train_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                             batch_size=params['batch_size'],\n",
    "                                             shuffle=False, \n",
    "                                             num_workers=params['num_workers'], \n",
    "                                             )\n",
    "\n",
    "save = os.path.join(params['model_save_path'])\n",
    "model=MyModel(768,5)\n",
    "model = model.to(params[\"device\"])\n",
    "model.load_state_dict(torch.load(os.path.join(save, \"200_model.pt\")))\n",
    "acc, preds, gt = inference(test_loader, model, params)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e09982a3-b460-4e87-bdc2-a1452877afc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at owkin/phikon were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[247  52  87   0  39]\n",
      " [103 542 467   3  75]\n",
      " [ 36  78 509   0  32]\n",
      " [  1   2  24  15   3]\n",
      " [  9   3  60   0 938]]\n",
      "0.6769924812030075\n"
     ]
    }
   ],
   "source": [
    "model=MyModel(768,5)\n",
    "model = model.to(params[\"device\"])\n",
    "model.load_state_dict(torch.load(os.path.join(save, \"102_model.pt\")))\n",
    "acc, preds, gt = inference(test_loader, model, params)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52fe2f65-be1e-4560-b124-0d973a10e7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at owkin/phikon were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[229  92  85   0  19]\n",
      " [ 18 797 345   2  28]\n",
      " [  4  51 586   0  14]\n",
      " [  1   7  22  15   0]\n",
      " [  2  21  68   0 919]]\n",
      "0.7657142857142857\n"
     ]
    }
   ],
   "source": [
    "model=MyModel(768,5)\n",
    "model = model.to(params[\"device\"])\n",
    "model.load_state_dict(torch.load(os.path.join(save, \"250_model.pt\")))\n",
    "acc, preds, gt = inference(test_loader, model, params)\n",
    "print(acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5766efca-f981-4d12-b705-3da15d6de087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.58      0.60       425\n",
      "           1       0.80      0.46      0.58      1190\n",
      "           2       0.44      0.78      0.56       655\n",
      "           3       0.83      0.33      0.48        45\n",
      "           4       0.86      0.93      0.89      1010\n",
      "\n",
      "    accuracy                           0.68      3325\n",
      "   macro avg       0.71      0.62      0.62      3325\n",
      "weighted avg       0.73      0.68      0.67      3325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "labels = ['G1','G2','G3','G4','Other']\n",
    "cm = confusion_matrix(gt, preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels = labels)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "report = classification_report(gt, preds)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7efdc92-642e-46af-a25a-739181bb387d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70      1615\n",
      "           1       0.47      0.78      0.59       700\n",
      "           2       0.86      0.93      0.89      1010\n",
      "\n",
      "    accuracy                           0.73      3325\n",
      "   macro avg       0.74      0.77      0.73      3325\n",
      "weighted avg       0.79      0.73      0.74      3325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_mapping = {\n",
    "    0: 0,  # Map class 0 to new class 0\n",
    "    1: 0,  # Map class 1 to new class 0\n",
    "    2: 1,  # Map class 2 to new class 1\n",
    "    3: 1,  # Map class 3 to new class 1\n",
    "    4: 2   # Map class 4 to new class 2\n",
    "}\n",
    "\n",
    "# Convert 5-class list to 3-class list\n",
    "new_gt = [class_mapping[class_id] for class_id in gt]\n",
    "new_preds = [class_mapping[class_id] for class_id in preds]\n",
    "\n",
    "labels = ['Low','High','Other']\n",
    "cm = confusion_matrix(new_gt, new_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels = labels)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()\n",
    "\n",
    "report = classification_report(new_gt, new_preds)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7bc0359a-f1a9-4643-b87b-5c304fda255c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at owkin/phikon were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[232  70 104   0  19]\n",
      " [ 11 716 431   0  32]\n",
      " [  4  29 607   0  15]\n",
      " [  0   5  24  15   1]\n",
      " [  1   8  59   0 942]]\n",
      "0.7554887218045113\n"
     ]
    }
   ],
   "source": [
    "model=MyModel(768,5)\n",
    "model = model.to(params[\"device\"])\n",
    "model.load_state_dict(torch.load(os.path.join(save, \"300_model.pt\")))\n",
    "acc, preds, gt = inference(test_loader, model, params)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e48994-9dff-4799-a1a3-0cf685eaec91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
